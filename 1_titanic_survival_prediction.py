# -*- coding: utf-8 -*-
"""1.TITANIC SURVIVAL PREDICTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_wAV66wXo1uZJ1Yd2yCxIVc7VN7HhWyA

**TITANIC SURVIVAL PREDICTION**

Introduction
Overview of the Titanic dataset
The Titanic dataset is a famous dataset in machine learning and data analysis, containing data on the passengers aboard the Titanic and their survival status. It consists of a training dataset with 891 observations and a test dataset with 418 observations. The goal is to predict the survival of passengers in the test dataset based on the variables in the training dataset.

Import libraries
To begin, let's import the necessary libraries that we'll be using throughout this notebook:
"""

# Importing standard libraries
import os
import re
import warnings

# Importing related third-party libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.compose import make_column_transformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler

"""Getting the Data"""

ds = pd.read_csv('/content/tested.csv')
ds.head()

"""Understanding the Variables
Before we can begin analyzing the Titanic dataset, it's important to understand what each variable represents. The dataset contains the following variables:

PassengerId: A unique identifier for each passenger.

Survived: Whether or not the passenger survived (0 = No, 1 = Yes).

Pclass: The passenger class (1 = 1st class, 2 = 2nd class, 3 = 3rd class).

Name: The name of the passenger.

Sex: The gender of the passenger.

Age: The age of the passenger in years. Fractional values are included for infants.

SibSp: The number of siblings/spouses aboard the Titanic.

Parch: The number of parents/children aboard the Titanic.

Ticket: The ticket number for the passenger.

Fare: The fare paid by the passenger.

Cabin: The cabin number for the passenger (if available).

Embarked: The port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton).

Data Exploration/Analysis
"""

ds.info()

ds.describe()

"""Above we can see that 36% out of the training-set survived the Titanic. On top of that we can already detect some features, that contain missing values, like the ‘Age’ feature."""

ds.head(8)

ds.tail(20)

def identify_missing_values(data):
    """Performs missing values computation."""
    # Verifying the presence of missing values
    miss_vals_num = data.isnull().sum()[data.isnull().sum() > 0]
    if miss_vals_num.empty:
        print("No missing values found.")
        return

    # Creating a table with numbers of missing values
    cols = {"missing_count": miss_vals_num.values}
    nans_df = pd.DataFrame(data=cols, index=miss_vals_num.index).sort_values(
        by="missing_count", ascending=False
    )

    # Adding shares of missing values
    nans_df["missing_fraction"] = nans_df["missing_count"] / data.shape[0]
    nans_df["missing_fraction"] = nans_df["missing_fraction"].round(4)

    # Adding data types
    nans_df["dtype"] = data[nans_df.index].dtypes
    nans_df = nans_df[["dtype", "missing_count", "missing_fraction"]]

    return nans_df
# Computing the number and share of

test_data=ds

"""Test data overview"""

# Displaying the summary of the data
test_data.info()

identify_missing_values(data=ds)

"""

Exploratory data analysis"""

# Setting the style for plots
sns.set_theme(style="darkgrid")

training_data=test_data

"""Training data"""

# Plotting histograms for the training data
cols_to_display = training_data.columns[1:]
training_data[cols_to_display].hist(
    figsize=(8, 10), color="green", edgecolor="black")
plt.suptitle("Numeric columns (train)", fontsize=15)
plt.tight_layout()
plt.show()

# Computing descriptive statistics
training_data.describe().round(2)

# Plotting barplots for categorical columns (training set)
fig, axes = plt.subplots(2, 1, figsize=(8, 10))

# Plotting number of passengers by sex
num_pass_sex = sns.countplot(data=training_data, x="Sex", ax=axes[0])
axes[0].set_title("Number of passengers by sex (train)", fontsize=15)
axes[0].set_ylabel("Number of people")

# Plotting number of passengers by embarkation port
num_pass_embarked = sns.countplot(data=training_data, x="Embarked", ax=axes[1])
axes[1].set_title(
    "Number of passengers by embarkation port (train)", fontsize=15
)
axes[1].set_ylabel("Number of people")

plt.tight_layout()
plt.show()

# Plotting histograms for the test data
cols_to_display = test_data.columns[1:]
test_data[cols_to_display].hist(
    figsize=(8, 10), color="orange", edgecolor="black"
)
plt.suptitle("Numeric columns (test)", fontsize=15)
plt.tight_layout()
plt.show()

from sklearn.base import BaseEstimator, TransformerMixin

class Eliminator(BaseEstimator, TransformerMixin):
    def __init__(self, features):
        self.features = features
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        Xtransformed = X.drop(self.features, axis=1, inplace=False)
        return Xtransformed

eliminated = ['Name', 'Ticket', 'Cabin']
el = Eliminator(eliminated)
el.fit_transform(ds)

"""
Design Custom Transformers¶
Eliminator





Transformer to drop specific columns. Accepts iteratable of strings features to be dropped."""

from sklearn.preprocessing import OrdinalEncoder

class Encoder(OrdinalEncoder):
    def __init__(self, cloumns_to_encode):
        self.columns = None
        self.cloumns_to_encode = cloumns_to_encode
        OrdinalEncoder.__init__(self)
    def fit(self, X, y=None):
        self.columns = X.columns
        OrdinalEncoder.fit(self, X[self.cloumns_to_encode], y)
        return self
    def transform(self, X):
        Xtransformed = X.copy()
        encoded_data = OrdinalEncoder.transform(self, X[self.cloumns_to_encode])
        Xtransformed[self.cloumns_to_encode] = encoded_data
        return Xtransformed

en = Encoder(['Sex', 'Embarked'])
en.fit_transform(ds)

"""Design Custom Transformers
Eliminator



Transformer to drop specific columns. Accepts iteratable of strings features to be dropped.
"""

from sklearn.base import BaseEstimator, TransformerMixin

class Eliminator(BaseEstimator, TransformerMixin):
    def __init__(self, features):
        self.features = features
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        Xtransformed = X.drop(self.features, axis=1, inplace=False)
        return Xtransformed

eliminated = ['Name', 'Ticket', 'Cabin']
el = Eliminator(eliminated)
el.fit_transform(ds)

"""
Encoder¶




Transformer to encode data to numerical discerete values of specific columns inheriting OrdinalEncoder utility. Accepts iteratable of strings cloumns_to_encode to be encoded."""

from sklearn.preprocessing import OrdinalEncoder

class Encoder(OrdinalEncoder):
    def __init__(self, cloumns_to_encode):
        self.columns = None
        self.cloumns_to_encode = cloumns_to_encode
        OrdinalEncoder.__init__(self)
    def fit(self, X, y=None):
        self.columns = X.columns
        OrdinalEncoder.fit(self, X[self.cloumns_to_encode], y)
        return self
    def transform(self, X):
        Xtransformed = X.copy()
        encoded_data = OrdinalEncoder.transform(self, X[self.cloumns_to_encode])
        Xtransformed[self.cloumns_to_encode] = encoded_data
        return Xtransformed

en = Encoder(['Sex', 'Embarked'])
en.fit_transform(ds)

def print_inf_and_nan_columns(dataframe):
    notfound = True
    for col in dataframe.columns:
        inf_count = np.isinf(dataframe[col]).values.ravel().sum()
        if inf_count != 0:
            notfound = False
            print(f"'{col}' has {inf_count} +/-inf values!")
        nan_count = np.isnan(dataframe[col]).values.ravel().sum()
        if nan_count != 0:
            notfound = False
            print(f"'{col}' has {nan_count} nan values!")
    if notfound:
        print('No column has nan or +/-inf.')

print_inf_and_nan_columns(en.transform(el.transform(ds)))

"""CustomImputer



Transformer to impute columns data inheriting SimpleImputer utility. CustomImputer does not do anything more than calling SimpleImputer functions and return transformed data as pandas.DataFrame instead of numpy.ndarray.
"""

from sklearn.impute import SimpleImputer

class CustomImputer(SimpleImputer):
    def __init__(self, missing_values=np.nan, strategy='mean'):
        self.columns = None
        SimpleImputer.__init__(self,
                               missing_values=missing_values,
                               strategy=strategy)
    def fit(self, X, y=None):
        self.columns = X.columns
        SimpleImputer.fit(self, X, y)
        return self
    def transform(self, X):
        Xtransformed = X.copy()
        Xtransformed = SimpleImputer.transform(self, Xtransformed)
        return pd.DataFrame(columns=self.columns, data=Xtransformed)

"""CustomSTDScaler¶


Transformer to impute scale columns data inheriting StandardScaler utility. Like CustomImputer, CustomSTDScaler does not do anything more than calling SimpleImputer functions and return transformed data as pandas.DataFrame instead of numpy.ndarray.
"""

from sklearn.preprocessing import StandardScaler

class CustomSTDScaler(StandardScaler):
    '''
    `StandardScaler` but returns pandas.DataFrame instead of numpy array when transform is invoked.
    '''
    def __init__(self, keep_features):
        self.columns = None
        self.keep_features = keep_features
        StandardScaler.__init__(self)
    def fit(self, X, y=None):
        self.columns = [col for col in X.columns if col not in self.keep_features]
        StandardScaler.fit(self, X[self.columns])
        return self
    def transform(self, X):
        Xtransformed = X.copy()
        Xtransformed[self.columns] = StandardScaler.transform(self, X[self.columns])
        return Xtransformed

"""Prepare Data with Preparing Pipeline"""

X, y = ds.drop('Survived', axis=1), ds['Survived']
X

DIV  = Operation(function=(lambda c1, c2 : c1 / c2),                separator='to')
MUL  = Operation(function=(lambda c1, c2 : c1 * c2),                separator='by')
CDIV = Operation(function=(lambda c1, c2 : DIV.function(c1, c2+1)), separator='to')

eliminated        = ['Name', 'Ticket', 'Cabin']
cloumns_to_encode = ['Sex', 'Embarked']
new_features      = (('Pclass', 'Fare', MUL), ('Parch', 'SibSp', CDIV))
keep_features     = ['PassengerId', 'Sex']

from sklearn.pipeline import Pipeline
preparing_pipeline = Pipeline([
    ('eliminator', Eliminator(eliminated)),
    ('encoder', Encoder(cloumns_to_encode)),
    ('synthesis_reactor', SynthesisReactor(params=new_features)),
    ('imputer', CustomImputer(strategy='mean')),
    ('scaler', CustomSTDScaler(keep_features))
])

prepared = preparing_pipeline.fit_transform(X)
prepared

"""Train and Evaluate Models"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

knn = KNeighborsClassifier()
knn.fit(prepared, y)
preds = knn.predict(prepared)
accuracy_score(y, preds)

from sklearn.svm import SVC

svc = SVC()
svc.fit(prepared, y)
preds_svc = svc.predict(prepared)
accuracy_score(y, preds_svc)

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()
rfc.fit(prepared, y)
preds_rfc = rfc.predict(prepared)
accuracy_score(y, preds_rfc)